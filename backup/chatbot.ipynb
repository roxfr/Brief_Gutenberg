{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CPU\n",
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu\n",
    "# GPU (CUDA)\n",
    "# !pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPU\n",
    "# !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu\n",
    "# CPU\n",
    "# !pip install llama-cpp-python\n",
    "# !wget https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.1/llama_cpp_python-0.3.1-cp310-cp310-win_amd64.whl\n",
    "# !pip install llama_cpp_python-0.3.1-cp310-cp310-win_amd64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment\n",
    "# !set FORCE_CMAKE=1\n",
    "# !set CMAKE_ARGS=-DLLAMA_CUBLAS=ON\n",
    "# !set HF_HUB_OFFLINE = 1\n",
    "# !set HF_DATASETS_OFFLINE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Git LFS (Large File Storage)\n",
    "# !git lfs install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install transformers huggingface_hub\n",
    "# !pip install huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2-7B-Chat-GGUF\n",
    "# !git clone https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF ../models/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Llama-2-13b-chat-GGUF\n",
    "# !git clone https://huggingface.co/TheBloke/Llama-2-13b-chat-GGUF ../models/Llama-2-13b-chat-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mixtral-8x22B-v0.1.IQ1_M.gguf\n",
    "#!wget \"https://huggingface.co/MaziyarPanahi/Mixtral-8x22B-v0.1-GGUF/resolve/main/Mixtral-8x22B-v0.1.IQ1_M.gguf\" -O ../models/Mixtral-8x22B-v0.1.IQ1_M.gguf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import pandas as pd\n",
    "from tqdm.autonotebook import tqdm\n",
    "from typing import Tuple\n",
    "# Langchain\n",
    "from langchain.llms import LlamaCpp\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "#from langchain_core.embeddings import Embeddings\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "# Embeddings\n",
    "from sentence_transformers import SentenceTransformer\n",
    "# Chroma\n",
    "import chromadb\n",
    "from langchain_chroma import Chroma\n",
    "from chromadb import PersistentClient, Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from config import load_config\n",
    "config = load_config()\n",
    "CSV_CLEANED_PATH = config[\"CSV_CLEANED_PATH\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path: str) -> pd.DataFrame:\n",
    "    \"\"\"Charge le fichier CSV et retourne un DataFrame.\"\"\"\n",
    "    try:\n",
    "        return pd.read_csv(file_path, sep=';')\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors du chargement du fichier CSV : {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWrapper:\n",
    "    def embed_documents(self, texts: list) -> list:\n",
    "        \"\"\"Encode les documents et retourne les embeddings.\"\"\"\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc=\"Encoding documents\"):\n",
    "            embeddings.append(embeddings_model.encode(text))\n",
    "        return embeddings\n",
    "    def embed_query(self, query: str) -> list:\n",
    "        \"\"\"Encode une requête et retourne l'embedding.\"\"\"\n",
    "        return embeddings_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EmbeddingWrapper:\n",
    "#     def embed_documents(self, texts):\n",
    "#         embeddings = []\n",
    "#         for text in tqdm(texts, desc=\"Encoding documents\"):\n",
    "#             embeddings.append(embeddings_model.encode(text))\n",
    "#         return embeddings\n",
    "\n",
    "#     def embed_query(self, query):\n",
    "#         return embeddings_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_existing_vector_store(persist_directory: str, collection_name: str) -> Tuple[Chroma, bool, PersistentClient]:\n",
    "    \"\"\"Récupère un magasin de vecteurs existant.\"\"\"\n",
    "    persistent_client = PersistentClient(path=persist_directory, settings=Settings(anonymized_telemetry=False))\n",
    "    \n",
    "    try:\n",
    "        collection = persistent_client.get_collection(collection_name)\n",
    "        logging.info(\"Collection existante chargée.\")\n",
    "        return collection, True, persistent_client\n",
    "    except Exception:\n",
    "        logging.warning(\"Collection inexistante.\")\n",
    "        return None, False, persistent_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(data: pd.DataFrame, batch_size: int = 10, persist_directory: str = \"../models/chroma_langchain_db\", collection_name: str = \"project_gutenberg\") -> Chroma:\n",
    "    \"\"\"Crée un magasin de vecteurs à partir des données.\"\"\"\n",
    "    collection, exists, persistent_client = get_existing_vector_store(persist_directory, collection_name)\n",
    "    \n",
    "    if not exists:\n",
    "        logging.info(\"Création d'une nouvelle collection...\")\n",
    "        collection = persistent_client.create_collection(collection_name)\n",
    "    else:\n",
    "        return Chroma(\n",
    "            client=persistent_client,\n",
    "            collection_name=collection_name,\n",
    "            embedding_function=EmbeddingWrapper(),\n",
    "        )\n",
    "\n",
    "    existing_ids = set()\n",
    "    try:\n",
    "        existing_docs = collection.get()['documents']\n",
    "        existing_ids = {doc['id'] for doc in existing_docs}\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Erreur lors de la récupération des documents existants : {e}\")\n",
    "\n",
    "    titles = data['Title'].tolist()\n",
    "    authors = data['Author'].tolist()\n",
    "    summaries = data['Summary'].tolist()\n",
    "    \n",
    "    logging.info(\"Génération d'IDs uniques...\")\n",
    "    unique_ids = []\n",
    "    seen_ids = set()\n",
    "    \n",
    "    for idx in range(len(titles)):\n",
    "        unique_id = idx + 1\n",
    "        while unique_id in seen_ids or (exists and unique_id in existing_ids):\n",
    "            unique_id += 1\n",
    "        unique_ids.append(unique_id)\n",
    "        seen_ids.add(unique_id)\n",
    "\n",
    "    logging.info(\"Ajout ou mise à jour des textes dans le magasin de vecteurs...\")\n",
    "    \n",
    "    for i in tqdm(range(0, len(summaries), batch_size), desc=\"Traitement des lots\"):\n",
    "        try:\n",
    "            metadata = [{'author': authors[j], 'ebook_no': str(data.iloc[j]['EBook-No.'])} for j in range(i, min(i + batch_size, len(summaries)))]\n",
    "            if collection is not None:\n",
    "                collection.add(ids=unique_ids[i:i + batch_size], documents=summaries[i:i + batch_size], metadatas=metadata)\n",
    "            else:\n",
    "                logging.error(\"La collection est None, impossible d'ajouter les documents.\")\n",
    "        except Exception as e:\n",
    "            logging.error(f\"Erreur lors de l'ajout ou de la mise à jour des textes au lot {i // batch_size}: {e}\")\n",
    "\n",
    "    logging.info(f\"{len(summaries)} textes ajoutés ou mis à jour dans le magasin de vecteurs.\")\n",
    "    \n",
    "    return Chroma(\n",
    "        client=persistent_client,\n",
    "        collection_name=collection_name,\n",
    "        embedding_function=EmbeddingWrapper(),\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import chromadb\n",
    "# from langchain_chroma import Chroma\n",
    "# from chromadb import PersistentClient, Settings\n",
    "\n",
    "# persist_directory = \"../models/chroma_langchain_db\"\n",
    "# client = chromadb.PersistentClient(path=persist_directory, settings=Settings(anonymized_telemetry=False))\n",
    "# collection_name = \"project_gutenberg\"\n",
    "# collection = client.get_collection(collection_name)\n",
    "\n",
    "# def display_collection_documents(collection, num_documents=10):\n",
    "#     \"\"\"Affiche les documents d'une collection ainsi que leurs métadonnées et IDs.\"\"\"\n",
    "#     documents = collection.get()\n",
    "#     for i, (doc, meta, doc_id) in enumerate(zip(documents['documents'], documents['metadatas'], documents['ids'])):\n",
    "#         if i >= num_documents:\n",
    "#             break\n",
    "#         print(f\"Document {i + 1}:\")\n",
    "#         print(f\"  ID: {doc_id}\")\n",
    "#         print(f\"  Texte: {doc}\")\n",
    "#         print(f\"  Métadonnées: {meta}\\n\")\n",
    "\n",
    "# display_collection_documents(collection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vector_store(data: pd.DataFrame, batch_size: int = 10) -> Chroma:\n",
    "#     \"\"\"Crée un magasin de vecteurs à partir des données ou le charge s'il existe déjà.\"\"\"\n",
    "#     persist_directory = \"../models/chroma_langchain_db\"\n",
    "#     if os.path.exists(persist_directory):\n",
    "#         logging.info(\"Chargement du vector store existant...\")\n",
    "#         vector_store = Chroma(\n",
    "#             embedding_function=EmbeddingWrapper(),\n",
    "#             persist_directory=persist_directory\n",
    "#         )\n",
    "#         return vector_store\n",
    "#     titles = data['Title'].tolist()\n",
    "#     authors = data['Author'].tolist()\n",
    "#     summaries = data['Summary'].tolist()\n",
    "#     unique_ids = []\n",
    "#     seen_ids = set()\n",
    "#     for title, author in zip(titles, authors):\n",
    "#         base_id = f\"{title}_{author}\"\n",
    "#         unique_id = base_id\n",
    "#         counter = 1\n",
    "#         while unique_id in seen_ids:\n",
    "#             unique_id = f\"{base_id}_{counter}\"\n",
    "#             counter += 1\n",
    "#         unique_ids.append(unique_id)\n",
    "#         seen_ids.add(unique_id)\n",
    "#     embedding_function = EmbeddingWrapper()\n",
    "#     vector_store = Chroma(\n",
    "#         embedding_function=embedding_function,\n",
    "#         persist_directory=persist_directory\n",
    "#     )\n",
    "#     logging.info(\"Ajout des textes au magasin de vecteurs par lots...\")\n",
    "#     for i in tqdm(range(0, len(summaries), batch_size), desc=\"Processing batches\"):\n",
    "#         try:\n",
    "#             metadata = [{'author': authors[j], 'ebook_no': data.iloc[j]['EBook-No.']} for j in range(i, min(i + batch_size, len(summaries)))]\n",
    "#             vector_store.add_texts(summaries[i:i + batch_size], ids=unique_ids[i:i + batch_size], metadata=metadata)\n",
    "#         except Exception as e:\n",
    "#             logging.error(f\"Erreur lors de l'ajout des textes au lot {i // batch_size}: {e}\")\n",
    "#     logging.info(f\"{len(summaries)} textes ajoutés au magasin de vecteurs.\")\n",
    "#     return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_vector_store(data):\n",
    "#     summaries = data['Summary'].tolist()\n",
    "#     embedding_function = EmbeddingWrapper()\n",
    "#     vector_store = Chroma(\n",
    "#         collection_name=\"gutenberg_books\",\n",
    "#         embedding_function=embedding_function,\n",
    "#         persist_directory=\"../models/chroma_langchain_db\",\n",
    "#         )\n",
    "#     vector_store.add_texts(summaries)\n",
    "#     return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_llama(model_path: str = \"../models/Llama-2-13B-Chat-GGUF/llama-2-13b-chat.Q8_0.gguf\",\n",
    "                    n_gpu_layers: int = 40, n_batch: int = 512,\n",
    "                    temperature: float = 0.7, max_tokens: int = 150) -> LlamaCpp:\n",
    "    \"\"\"Configure et retourne le modèle Llama.\"\"\"\n",
    "    #model_path = os.path.abspath(\"../models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\")\n",
    "    model_path = os.path.abspath(\"../models/Llama-2-13B-Chat-GGUF/llama-2-13b-chat.Q8_0.gguf\")\n",
    "    logging.info(f\"Loading Llama model from: {model_path}\")\n",
    "    try:\n",
    "        llm = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=n_gpu_layers,\n",
    "            n_batch=n_batch,\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            verbose=True,\n",
    "        )\n",
    "        logging.info(\"Llama model loaded successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Error loading Llama model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_author(vector_store: Chroma, title: str, data: pd.DataFrame) -> str:\n",
    "    \"\"\"Recherche l'auteur d'un livre par son titre à partir du vector store ou du DataFrame en cas d'échec.\"\"\"\n",
    "    logging.info(f\"Recherche de l'auteur pour : {title}\")\n",
    "    results = vector_store.query(title, k=1)\n",
    "    if results and len(results) > 0:\n",
    "        return results[0]['author']\n",
    "    # Fallback vers le DataFrame si aucune correspondance trouvée dans le vector store\n",
    "    logging.info(\"Aucun résultat trouvé dans le vector store, recherche dans le DataFrame...\")\n",
    "    row = data[data['Title'].str.contains(title, case=False)]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['Author']\n",
    "    return \"Auteur non trouvé.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_subject(vector_store: Chroma, title: str, data: pd.DataFrame) -> str:\n",
    "    \"\"\"Retourne le sujet d'un livre par son titre à partir du vector store ou du DataFrame en cas d'échec.\"\"\"\n",
    "    logging.info(f\"Recherche du sujet pour : {title}\")\n",
    "    results = vector_store.query(title, k=1)\n",
    "    if results and len(results) > 0:\n",
    "        return results[0]['subject']\n",
    "    # Fallback vers le DataFrame si aucune correspondance trouvée dans le vector store\n",
    "    logging.info(\"Aucun résultat trouvé dans le vector store, recherche dans le DataFrame...\")\n",
    "    row = data[data['Title'].str.contains(title, case=False)]\n",
    "    if not row.empty:\n",
    "        return row.iloc[0]['Subject']\n",
    "    return \"Sujet non trouvé.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_characters(vector_store: Chroma, title: str, data: pd.DataFrame) -> str:\n",
    "    \"\"\"Extrait les personnages d'un livre par son titre à partir du résumé dans le vector store ou le DataFrame en cas d'échec.\"\"\"\n",
    "    logging.info(f\"Recherche du/des personnage(s) pour : {title}\")\n",
    "    results = vector_store.query(title, k=1)\n",
    "    if results and len(results) > 0:\n",
    "        summary = results[0]['summary']\n",
    "    else:\n",
    "        # Fallback vers le DataFrame si aucune correspondance trouvée dans le vector store\n",
    "        logging.info(\"Aucun résultat trouvé dans le vector store, recherche dans le DataFrame...\")\n",
    "        row = data[data['Title'].str.contains(title, case=False)]\n",
    "        if not row.empty:\n",
    "            summary = row.iloc[0]['Summary']\n",
    "        else:\n",
    "            return [\"Aucun personnage cité.\"]\n",
    "    characters = set(word for word in summary.split() if word.istitle())\n",
    "    if characters:\n",
    "        return list(characters)\n",
    "    else:\n",
    "        return [\"Aucun personnage cité.\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_text_from_url(vector_store: Chroma, title: str, data: pd.DataFrame) -> str:\n",
    "    \"\"\"Récupère le texte complet d'un livre en ligne par son titre en utilisant le vector store ou le DataFrame en cas d'échec.\"\"\"\n",
    "    logging.info(f\"Recherche du texte en ligne pour : {title}\")    \n",
    "    query_embedding = vector_store.embed(title)\n",
    "    results = vector_store.query(query_embedding)\n",
    "    if results and len(results) > 0:\n",
    "        best_match = results[0]\n",
    "        ebook_no = best_match.get('metadata', {}).get('ebook_no')\n",
    "        if ebook_no:\n",
    "            url = f\"https://www.gutenberg.org/files/{ebook_no}/{ebook_no}-0.txt\"\n",
    "            try:\n",
    "                response = requests.get(url)\n",
    "                response.raise_for_status()\n",
    "                return response.text\n",
    "            except requests.exceptions.RequestException as e:\n",
    "                logging.error(f\"Erreur lors de la récupération du texte : {e}\")\n",
    "                return f\"Erreur lors de la récupération du texte : {e}\"\n",
    "    # Fallback vers le DataFrame si aucune correspondance trouvée dans le vector store\n",
    "    logging.info(f\"Titre non trouvé dans le vector store, recherche dans le DataFrame : {title}\")\n",
    "    ebook_no_row = data[data['Title'].str.contains(title, case=False)]\n",
    "    if not ebook_no_row.empty:\n",
    "        ebook_no = ebook_no_row.iloc[0]['EBook-No.']\n",
    "        url = f\"https://www.gutenberg.org/files/{ebook_no}/{ebook_no}-0.txt\"\n",
    "        try:\n",
    "            response = requests.get(url)\n",
    "            response.raise_for_status()\n",
    "            return response.text\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            logging.error(f\"Erreur lors de la récupération du texte : {e}\")\n",
    "            return f\"Erreur lors de la récupération du texte : {e}\"\n",
    "    return \"Livre non trouvé.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = {\n",
    "    \"get_author\": \"Fonction pour trouver l'auteur d'un livre\",\n",
    "    \"get_subject\": \"Fonction pour obtenir le sujet d'un livre\",\n",
    "    \"get_characters\": \"Fonction pour extraire les personnages d'un livre\",\n",
    "    \"get_full_text_from_url\": \"Fonction pour récupérer le texte intégral ou complet d'un livre\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_qa_chain(llm: LlamaCpp, vector_store: Chroma) -> RunnableSequence:\n",
    "    \"\"\"Mise en place de la chaîne QA.\"\"\"\n",
    "    template = \"\"\"\n",
    "    Cet agent répond à des questions sur des livres du projet Gutenberg. \n",
    "    L'utilisateur a posé la question : '{question}'.\n",
    "    Outils disponibles : {tools}.\n",
    "    N'oubli pas l'outil get_full_text_from_url pour récupérer le texte intégral ou complet d'un livre\n",
    "    Fournir une réponse en une phrase concise, directe, précise sans mentionner les outils ou reformuler la question.\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\", \"tools\"])\n",
    "    retriever = vector_store.as_retriever(k=1)\n",
    "    qa_chain = RunnableSequence(\n",
    "        {\n",
    "            \"context\": retriever, \n",
    "            \"question\": RunnablePassthrough(),\n",
    "            \"tools\": RunnablePassthrough()\n",
    "        }\n",
    "        | prompt\n",
    "        | llm\n",
    "    )    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def setup_qa_chain(llm, vector_store):\n",
    "#     template = \"\"\"Contexte : Cet agent, basé sur un modèle de langage (LLM), \n",
    "#     a pour objectif de répondre à des questions sur des livres provenant de Project Gutenberg. \n",
    "#     Il est capable d'extraire des informations sur les livres et leurs personnages, \n",
    "#     ainsi que d'interagir avec le texte complet d'un livre. \n",
    "\n",
    "#     Vous êtes un assistant intelligent et bien informé sur les ouvrages du projet Gutenberg. \n",
    "#     L'utilisateur a posé la question suivante : '{question}'. \n",
    "#     Vous avez accès aux outils suivants : {tools}. \n",
    "\n",
    "#     Voici quelques actions que vous pouvez effectuer :\n",
    "#     1. Trouver l'auteur du livre.\n",
    "#     2. Donner le sujet du livre.\n",
    "#     3. Extraire les personnages du résumé.\n",
    "#     4. Récupérer le texte complet d'un livre.\n",
    "\n",
    "#     Question : {question}\n",
    "#     Réponse : Pour répondre à votre question, examinons d'abord les éléments clés et les détails pertinents. \n",
    "#     Voici ce que nous savons :\n",
    "#     \"\"\"\n",
    "#     prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "#     retriever = vector_store.as_retriever(k=5)\n",
    "#     qa_chain = RunnableSequence(\n",
    "#         {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "#         | prompt\n",
    "#         | llm\n",
    "#     )    \n",
    "#     return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_csv(CSV_CLEANED_PATH)\n",
    "logging.info(f\"Colonnes du DataFrame : {data.columns.tolist()}\")\n",
    "llm = configure_llama()\n",
    "vector_store = create_vector_store(data)\n",
    "qa_chain = setup_qa_chain(llm, vector_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = [\n",
    "        \"Qui est l'auteur du livre 'L'Assommoir' ?\",\n",
    "        \"Quel sujet est traité dans 'House of Atreus' ?\",\n",
    "        \"Qui sont les personnages principaux dans 'Uninhabited House' ?\",\n",
    "        \"Quel(s) sont le(s) titre(s) de(s) livres(s) de l'auteur Dickens Charles ?\",\n",
    "        \"Peux-tu me donner le texte intégral de 'Blue Bird' ?\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    try:\n",
    "        logging.info(f\"Outils : {list(tools.keys())}\")\n",
    "        for question in questions:\n",
    "            response = qa_chain.invoke({\"question\": question, \"tools\": list(tools.keys())})\n",
    "            logging.info(f\"Outils : {list(tools.keys())}\\n\")\n",
    "            logging.info(f\"Question : {question}\\n\")\n",
    "            logging.info(f\"Réponse : {response}\\n\")\n",
    "    except Exception as e:\n",
    "        logging.error(f\"Une erreur est survenue : {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Main\n",
    "# if __name__ == \"__main__\":\n",
    "#     file_path = os.path.join(\"../data\", \"gutenberg_cleaned.csv\")\n",
    "#     data = load_csv(file_path)\n",
    "#     print(data.columns)  \n",
    "#     llm = configure_llama()\n",
    "#     vector_store = create_vector_store(data)\n",
    "#     qa_chain = setup_qa_chain(llm, vector_store)\n",
    "#     question = \"Qui est l'auteur du livre Paradise Lost ?\"\n",
    "#     response = qa_chain.invoke({\"question\": question})\n",
    "#     print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutenberg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
