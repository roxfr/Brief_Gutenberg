{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "def load_config():\n",
    "    load_dotenv()\n",
    "    config = {\n",
    "        \"CSV_INPUT_PATH\": os.path.join(\"../datas\", \"gutenberg.csv\"),\n",
    "        \"CSV_CLEANED_PATH\": os.path.join(\"../datas\", \"gutenberg2.csv\"),\n",
    "        \"EMBEDDINGS_FILE\": os.path.join(\"../models\", \"embeddings.csv\"),\n",
    "        \"DOCS_FILE\": os.path.join(\"../models\", \"docs.csv\"),\n",
    "        \"FAISS_INDEX\": os.path.join(\"../models\", \"faiss_index.index\"),\n",
    "        \"CHUNK_SIZE\": int(500),\n",
    "        \"AZURE_OPENAI_API_KEY\": os.getenv(\"AZURE_OPENAI_API_KEY\"),\n",
    "        \"AZURE_OPENAI_API_BASE\": os.getenv(\"AZURE_OPENAI_API_BASE\"), \n",
    "        \"AZURE_OPENAI_API_ENDPOINT\": os.getenv(\"AZURE_OPENAI_API_ENDPOINT\"),\n",
    "        \"AZURE_DEPLOYMENT_NAME\": os.getenv(\"AZURE_DEPLOYMENT_NAME\"),\n",
    "        \"AZURE_API_VERSION\": os.getenv(\"AZURE_API_VERSION\"),\n",
    "        \"AZURE_DEPLOYEMENT\": os.getenv(\"AZURE_DEPLOYEMENT\"),\n",
    "        \"LANGCHAIN_ENDPOINT\": os.getenv(\"LANGCHAIN_ENDPOINT\"),\n",
    "        \"LANGCHAIN_API_KEY\": os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "    }\n",
    "    return config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai.embeddings import AzureOpenAIEmbeddings\n",
    "\n",
    "\n",
    "config = load_config()\n",
    "AZURE_OPENAI_API_BASE = config[\"AZURE_OPENAI_API_BASE\"]\n",
    "AZURE_OPENAI_API_KEY = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_DEPLOYEMENT = config[\"AZURE_DEPLOYEMENT\"]\n",
    "CHUNK_SIZE = config[\"CHUNK_SIZE\"]\n",
    "\n",
    "class EmbeddingModel:\n",
    "    def __init__(self):\n",
    "        self.embedding_model = AzureOpenAIEmbeddings(\n",
    "            azure_endpoint=AZURE_OPENAI_API_BASE,\n",
    "            openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "            azure_deployment=AZURE_DEPLOYEMENT,\n",
    "            chunk_size=CHUNK_SIZE\n",
    "        )\n",
    "    def get_embedding_model(self):\n",
    "        return self.embedding_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(EmbeddingModel().get_embedding_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "config = load_config()\n",
    "AZURE_OPENAI_API_ENDPOINT = config[\"AZURE_OPENAI_API_ENDPOINT\"]\n",
    "AZURE_DEPLOYMENT_NAME = config[\"AZURE_DEPLOYMENT_NAME\"]\n",
    "AZURE_OPENAI_API_KEY = config[\"AZURE_OPENAI_API_KEY\"]\n",
    "AZURE_API_VERSION = config[\"AZURE_API_VERSION\"]\n",
    "\n",
    "class LanguageModel:\n",
    "    def __init__(self):\n",
    "        self.llm = AzureChatOpenAI(\n",
    "            azure_endpoint=AZURE_OPENAI_API_ENDPOINT,\n",
    "            azure_deployment=AZURE_DEPLOYMENT_NAME,\n",
    "            openai_api_key=AZURE_OPENAI_API_KEY,\n",
    "            api_version=AZURE_API_VERSION,\n",
    "            temperature=0.0\n",
    "        )\n",
    "    def get_language_model(self):\n",
    "        return self.llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(LanguageModel().get_language_model())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import logging\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "EMBEDDINGS_FILE = os.path.join(\"../models\", \"embeddings.csv\")\n",
    "FAISS_INDEX = os.path.join(\"../models\", \"faiss_index.index\")\n",
    "DOCS_FILE = os.path.join(\"../models\", \"docs.csv\")\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def save_embeddings(embeddings):\n",
    "    df = pd.DataFrame(embeddings)\n",
    "    df.to_csv(EMBEDDINGS_FILE, index=False, header=False)\n",
    "\n",
    "def load_embeddings():\n",
    "    \"\"\"Charge le modèle d'embeddings depuis un fichier.\"\"\"\n",
    "    if os.path.exists(EMBEDDINGS_FILE):\n",
    "        df = pd.read_csv(EMBEDDINGS_FILE, header=None)\n",
    "        logger.info(f\"Modèle d'embedding chargé depuis {EMBEDDINGS_FILE}.\")\n",
    "        return df.values.tolist()\n",
    "    else:\n",
    "        logger.warning(\"Fichier d'embeddings non trouvé.\")\n",
    "        return EmbeddingModel()\n",
    "\n",
    "def save_vector_store(vector_store):\n",
    "    faiss.write_index(vector_store.index, FAISS_INDEX)\n",
    "    logger.info(f\"Magasin de vecteurs sauvegardé dans {FAISS_INDEX}.\")\n",
    "\n",
    "def load_vector_store(embedding_model):\n",
    "    if os.path.exists(FAISS_INDEX):\n",
    "        index = faiss.read_index(FAISS_INDEX)\n",
    "        docstore = InMemoryDocstore()\n",
    "        index_to_docstore_id = {}\n",
    "        vector_store = FAISS(\n",
    "            embedding_function=embedding_model.embed_query,\n",
    "            index=index,\n",
    "            docstore=docstore,\n",
    "            index_to_docstore_id=index_to_docstore_id\n",
    "        )        \n",
    "        logger.info(f\"Magasin de vecteurs chargé depuis {FAISS_INDEX}.\")\n",
    "        return vector_store\n",
    "    else:\n",
    "        logger.warning(\"Répertoire de sauvegarde non trouvé.\")\n",
    "        return None\n",
    "\n",
    "def save_docs(docs):\n",
    "    \"\"\"Sauvegarde les documents dans un fichier CSV.\"\"\"\n",
    "    df = pd.DataFrame(docs)\n",
    "    df.to_csv(DOCS_FILE, index=False, header=['id', 'content'])\n",
    "    logger.info(f\"Documents sauvegardés dans {DOCS_FILE}.\")\n",
    "\n",
    "def load_docs():\n",
    "    \"\"\"Charge les documents depuis un fichier CSV.\"\"\"\n",
    "    if os.path.exists(DOCS_FILE):\n",
    "        df = pd.read_csv(DOCS_FILE)  \n",
    "        logger.info(f\"Documents chargés depuis {DOCS_FILE}.\")\n",
    "        return df.to_dict(orient='records')\n",
    "    else:\n",
    "        logger.warning(\"Fichier de documents non trouvé.\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from uuid import uuid4\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import CSVLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def load_data(csv_path):\n",
    "    loader = CSVLoader(file_path=csv_path, encoding='utf-8')\n",
    "    return loader.load()\n",
    "\n",
    "def split_documents(data):\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=CHUNK_SIZE, chunk_overlap=0)\n",
    "    return text_splitter.split_documents(data)\n",
    "\n",
    "def create_vector_store(all_splits, embedding_model):\n",
    "    index = faiss.IndexFlatL2(len(embedding_model.embed_query(all_splits[0].page_content)))\n",
    "    docstore = InMemoryDocstore()\n",
    "    index_to_docstore_id = {}\n",
    "    vectorstore = FAISS(\n",
    "        embedding_function=embedding_model.embed_query, \n",
    "        index=index, \n",
    "        docstore=docstore, \n",
    "        index_to_docstore_id=index_to_docstore_id\n",
    "    )\n",
    "    total_docs = len(all_splits)\n",
    "    embeddings = []\n",
    "    docs = []\n",
    "    for i, split in enumerate(all_splits):\n",
    "        embedding = embedding_model.embed_query(split.page_content)\n",
    "        embeddings.append(embedding)\n",
    "        doc_id = str(uuid4())\n",
    "        vectorstore.add_documents([split], ids=[doc_id])\n",
    "        index_to_docstore_id[doc_id] = len(embeddings) - 1        \n",
    "        docs.append({'id': doc_id, 'content': split.page_content})\n",
    "        logger.info(f\"{i + 1}/{total_docs}\")\n",
    "    save_embeddings(embeddings)\n",
    "    save_vector_store(vectorstore)\n",
    "    save_docs(docs)\n",
    "    return vectorstore\n",
    "\n",
    "def configure_qa_chain(llm_model, vectorstore):\n",
    "    return RetrievalQA.from_chain_type(llm=llm_model, retriever=vectorstore.as_retriever())\n",
    "\n",
    "def run_chat_loop(qa_chain, docs):\n",
    "    while True:\n",
    "        user_input = input(\"Votre question (ou tapez 'exit' pour quitter) : \")\n",
    "        if user_input.lower() == 'exit':\n",
    "            break\n",
    "        if len(user_input.strip()) < 5:\n",
    "            print(\"Veuillez poser une question plus détaillée.\")\n",
    "            continue\n",
    "        context = \"\"\n",
    "        if docs:\n",
    "            context = \"\\n\".join([doc['content'] for doc in docs])\n",
    "        try:\n",
    "            response = qa_chain.invoke({\"query\": user_input, \"context\": context})\n",
    "            print(\"Réponse :\", response.get('output', \"Aucune réponse générée.\"))\n",
    "        except Exception as e:\n",
    "            print(\"Désolé, une erreur s'est produite lors du traitement de votre question.\")\n",
    "\n",
    "def main():\n",
    "    global CSV_CLEANED_PATH, CHUNK_SIZE\n",
    "    CSV_CLEANED_PATH = os.path.join(\"../datas\", \"gutenberg2.csv\")\n",
    "    CHUNK_SIZE = int(500)\n",
    "    data = load_data(CSV_CLEANED_PATH)\n",
    "    all_splits = split_documents(data)\n",
    "    embeddings = load_embeddings()\n",
    "    if isinstance(embeddings, list):\n",
    "        logger.info(\"Embeddings chargés avec succès.\")\n",
    "        embedding_model = EmbeddingModel().get_embedding_model()\n",
    "    else:\n",
    "        logger.info(\"Aucun modèle d'embeddings trouvé, création d'un nouveau modèle.\")\n",
    "        embedding_model = embeddings\n",
    "    vectorstore = load_vector_store(embedding_model)\n",
    "    if vectorstore is None:\n",
    "        logger.info(\"Aucun magasin de vecteurs trouvé, création d'un nouveau magasin.\")\n",
    "        vectorstore = create_vector_store(all_splits, embedding_model)\n",
    "    else:\n",
    "        logger.info(\"Magasin de vecteurs chargé avec succès.\")\n",
    "    docs = load_docs()\n",
    "    if docs is None or len(docs) == 0:\n",
    "        logger.warning(\"Aucun document trouvé. Certaines fonctionnalités peuvent ne pas être disponibles.\")\n",
    "    llm_model = LanguageModel().get_language_model()\n",
    "    qa_chain = configure_qa_chain(llm_model, vectorstore)\n",
    "    run_chat_loop(qa_chain, docs)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutenberg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
