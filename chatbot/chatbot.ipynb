{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install llama-cpp-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git lfs install\n",
    "# !git clone https://huggingface.co/TheBloke/Llama-2-7B-Chat-GGUF ../models/Llama-2-7B-Chat-GGUF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !git clone --recursive -j8 https://github.com/abetlen/llama-cpp-python.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !set FORCE_CMAKE=1\n",
    "# !set CMAKE_ARGS=-DLLAMA_CUBLAS=ON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import langchain_community\n",
    "# print(dir(langchain_community))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Thierry\\miniconda3\\envs\\gutenberg\\lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from langchain_chroma import Chroma\n",
    "from langchain.llms import LlamaCpp\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain_core.runnables import RunnableSequence\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings_model = SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv(file_path):\n",
    "    return pd.read_csv(file_path, sep=';')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingWrapper:\n",
    "    def embed_documents(self, texts):\n",
    "        embeddings = []\n",
    "        for text in tqdm(texts, desc=\"Encoding documents\"):\n",
    "            embeddings.append(embeddings_model.encode(text))\n",
    "        return embeddings\n",
    "\n",
    "    def embed_query(self, query):\n",
    "        return embeddings_model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(data):\n",
    "    combined_texts = (data['Title'] + \" \" + data['Summary']).tolist()\n",
    "    embedding_function = EmbeddingWrapper()\n",
    "    vector_store = Chroma(\n",
    "        collection_name=\"gutenberg_books\",\n",
    "        embedding_function=embedding_function,\n",
    "        persist_directory=\"../models/chroma_langchain_db\",\n",
    "        )\n",
    "    vector_store.add_texts(combined_texts)\n",
    "    return vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def configure_llama():\n",
    "    model_path = os.path.abspath(\"../models/Llama-2-7B-Chat-GGUF/llama-2-7b-chat.Q4_K_M.gguf\")\n",
    "    print(f\"Loading Llama model from: {model_path}\")\n",
    "    try:\n",
    "        llm = LlamaCpp(\n",
    "            model_path=model_path,\n",
    "            n_gpu_layers=40,\n",
    "            n_batch=512,\n",
    "            verbose=True,\n",
    "        )\n",
    "        print(\"Llama model loaded successfully.\")\n",
    "        return llm\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading Llama model: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Mise en place de la chaîne QA\n",
    "def setup_qa_chain(llm, vector_store):\n",
    "    template = \"\"\"Contexte : Cet agent, basé sur un modèle de langage (LLM), \n",
    "    a pour objectif de répondre à des questions sur des livres provenant de Project Gutenberg. \n",
    "    Il est capable d'extraire des informations sur les livres et leurs personnages, \n",
    "    ainsi que d'interagir avec le texte complet d'un livre.\n",
    "    Question: {question}\n",
    "    Answer: Pour répondre à votre question, examinons d'abord les éléments clés et les détails pertinents. \n",
    "    Voici ce que nous savons :\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"question\"])\n",
    "    retriever = vector_store.as_retriever(k=5)\n",
    "    qa_chain = RunnableSequence(\n",
    "        {\"context\": retriever, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "    )    \n",
    "    return qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Author', 'Title', 'Summary', 'Subject', 'EBook-No.', 'Release Date'], dtype='object')\n",
      "Loading Llama model from: d:\\FORMATIONS\\SIMPLON\\COURS\\2024\\S40\\Brief_Gutenberg\\models\\Llama-2-7B-Chat-GGUF\\llama-2-7b-chat.Q4_K_M.gguf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "AVX = 1 | AVX2 = 1 | AVX512 = 0 | AVX512_VBMI = 0 | AVX512_VNNI = 0 | FMA = 1 | NEON = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 1 | SSE3 = 1 | SSSE3 = 0 | VSX = 0 | \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Llama model loaded successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Encoding documents:   0%|          | 0/5000 [00:00<?, ?it/s]c:\\Users\\Thierry\\miniconda3\\envs\\gutenberg\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:440: UserWarning: 1Torch was not compiled with flash attention. (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\aten\\src\\ATen\\native\\transformers\\cuda\\sdp_utils.cpp:555.)\n",
      "  attn_output = torch.nn.functional.scaled_dot_product_attention(\n",
      "Encoding documents: 100%|██████████| 5000/5000 [00:45<00:00, 109.56it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - L'auteur du livre \"Paradise Lost\" est John Milton.\n",
      "     - Le livre a été publié en 1667.\n",
      "     - Il s'agit d'une épopée religieuse qui raconte l'histoire de la chute d'Adam et Ève dans le jardin d'Éden.\n",
      "    En utilisant ces informations, nous pouvons donc conclure que John Milton est l'auteur du livre \"Paradise Lost\".\n",
      "    Nous espérons que cette réponse vous a été utile ! Si vous avez d'autres questions, n'hésitez pas à les poser.']\n",
      "Réponse de l'agent :\n",
      "L'auteur du livre \"Paradise Lost\" est John Milton.\n",
      "Dans cette réponse, l'agent utilise des phrases telles que \"examine d'abord les éléments clés et les détails pertinents\" pour souligner l'importance de ces éléments dans la résolution de la question. L'agent utilise également des termes de base tels que \"voici ce que nous savons\" pour\n"
     ]
    }
   ],
   "source": [
    "# Main\n",
    "if __name__ == \"__main__\":\n",
    "    file_path = os.path.join(\"../data\", \"gutenberg_cleaned.csv\")\n",
    "    data = load_csv(file_path)\n",
    "    print(data.columns)  \n",
    "    llm = configure_llama()\n",
    "    vector_store = create_vector_store(data)\n",
    "    qa_chain = setup_qa_chain(llm, vector_store)\n",
    "    question = \"Qui est l'auteur du livre Paradise Lost ?\"\n",
    "    response = qa_chain.invoke({\"question\": question})\n",
    "    print(response)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gutenberg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
